{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating embeddings\n",
    "\n",
    "First, I need to load and clean my corpus, then vectorize it. To create the embeddings, I will be using gensim [FastText](https://radimrehurek.com/gensim/models/fasttext.html) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sveta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sveta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "%run utility_file    # handles main module imports and loading .csv files\n",
    "from utility_file import Preprocess     # custom class for preprocessing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'pi2.csv'\n",
    "source_lang = 'English'\n",
    "target_lang = 'Russian'\n",
    "\n",
    "source_list, target_list = load_separate_corpora_from_csv(path, source_lang, target_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptions = [\n",
    "    'flowerbed', \n",
    "    'building'\n",
    "    ]       # words not handled by NLP libraries in the desired way\n",
    "russian_stopwords = stopwords.words('russian') + [\n",
    "    'это',\n",
    "    'ещё',\n",
    "    'ваш',\n",
    "    'всё',\n",
    "    'весь',\n",
    "    '-'\n",
    "    ]\n",
    "\n",
    "# Preprocessing the corpora\n",
    "\n",
    "clean_en_corpus = [Preprocess(sentence).preprocess_en_text(exceptions) for sentence in source_list]\n",
    "clean_ru_corpus = [Preprocess(sentence).preprocess_ru_text(russian_stopwords) for sentence in target_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickling the clean corpora for use in other notebooks\n",
    "\n",
    "import pickle\n",
    "with open('clean_en_corpus.pkl', 'wb') as f:\n",
    "       pickle.dump(clean_en_corpus, f)\n",
    "with open('clean_ru_corpus.pkl', 'wb') as f:\n",
    "       pickle.dump(clean_ru_corpus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loading pickled corpora\n",
    "\n",
    "# import pickle\n",
    "# with open('clean_en_corpus.pkl', 'rb') as f:\n",
    "#        clean_en_corpus = pickle.load(f)\n",
    "# with open('clean_ru_corpus.pkl', 'rb') as f:\n",
    "#        clean_ru_corpus = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using nltk's word tokenizer to tokenize every item in the clean corpora in order to feed them to the model\n",
    "\n",
    "word_tokenized_en_corpus = [word_punctuation_tokenizer.tokenize(sent) for sent in clean_en_corpus]\n",
    "word_tokenized_ru_corpus = [word_punctuation_tokenizer.tokenize(sent) for sent in clean_ru_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training gensim FastText model to create embeddings\n",
    "\n",
    "embedding_size = 30\n",
    "window_size = 45\n",
    "min_word = 5\n",
    "down_sampling = 1e-2\n",
    "\n",
    "ft_en_model = FastText(word_tokenized_en_corpus,\n",
    "                      size=embedding_size,\n",
    "                      window=window_size,\n",
    "                      min_count=min_word,\n",
    "                      sample=down_sampling,\n",
    "                      sg=1,\n",
    "                      iter=100)\n",
    "\n",
    "ft_ru_model = FastText(word_tokenized_ru_corpus,\n",
    "                      size=embedding_size,\n",
    "                      window=window_size,\n",
    "                      min_count=min_word,\n",
    "                      sample=down_sampling,\n",
    "                      sg=1,\n",
    "                      iter=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking out the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.44251978 -0.8151118   0.08512311  0.41910678  0.1451333  -0.12282278\n",
      "  0.5808773   0.15731221  0.08658551  1.0206383   0.7039301   1.3919924\n",
      " -0.41332284 -0.25333408  0.943905   -0.3329253  -0.38998294  0.04191609\n",
      "  0.20740989  0.10418067  1.1407379   0.0242559  -0.14824784 -0.30736986\n",
      "  0.07419863 -0.56922174 -0.94646376 -0.13597773 -0.06863962 -0.65130246]\n"
     ]
    }
   ],
   "source": [
    "# taking a look at the embeddings\n",
    "\n",
    "print(ft_en_model.wv['travel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crystal:['bank', 'refresh', 'buy', 'purchase', 'resource']\n",
      "naomi:['come', 'mano', 'could', 'soon', 'think']\n",
      "key:['lock', 'hunter', 'bury', 'treasure', 'purr']\n",
      "island:['visit', 'come', 'return', 'resort', 'arrive']\n",
      "build:['upgrade', 'unique', 'hotel', 'building', 'majority']\n",
      "reward:['prize', 'get', 'awesome', 'earn', 'receive']\n"
     ]
    }
   ],
   "source": [
    "semantically_similar_words = {words: [item[0] for item in ft_en_model.wv.most_similar([words], topn=5)]\n",
    "                  for words in [\n",
    "                      'crystal', \n",
    "                      'naomi', \n",
    "                      'key', \n",
    "                      'island', \n",
    "                      'build', \n",
    "                      'reward'\n",
    "                  ]}   \n",
    "                    # these are some commonly used game terms \n",
    "\n",
    "for k,v in semantically_similar_words.items():\n",
    "    print(k+\":\"+str(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first word in each list looks pretty good and indeed often appears in the same context as the key word. Below I'm checking cosine similarity between other terms frequently appearing in the same context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8523123\n",
      "0.7713198\n",
      "0.58982426\n"
     ]
    }
   ],
   "source": [
    "print(ft_en_model.wv.similarity(w1='dorin', w2='mano'))      # both are character names\n",
    "print(ft_en_model.wv.similarity(w1='event', w2='hotel'))     # events are held in hotels\n",
    "print(ft_en_model.wv.similarity(w1='gold', w2='coin'))       # coins are made of gold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and converting trained models for further use\n",
    "\n",
    "In order to perform vector space alignment in notebook 03, I will need .vec Facebook fasttext models. The code below saves our ready gensim models as .bin files and converts them to .vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import save_facebook_model\n",
    "save_facebook_model(ft_en_model, \"en_model_fb.bin\", encoding='utf-8')\n",
    "save_facebook_model(ft_ru_model, \"ru_model_fb.bin\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "from fasttext import load_model\n",
    "\n",
    "def convert_bin_to_vec(model, lang):\n",
    "    f = load_model(model)\n",
    "    lines=[]\n",
    "\n",
    "    # get all words from model\n",
    "    words = f.get_words()\n",
    "    name = lang + '_model_converted'\n",
    "    with open(\"%s.vec\" % name, 'w') as file_out:\n",
    "    \n",
    "    # the first line must contain number of total words and vector dimension\n",
    "        file_out.write(str(len(words)) + \" \" + str(f.get_dimension()) + \"\\n\")\n",
    "\n",
    "    # line by line, you append vectors to VEC file\n",
    "        for w in words:\n",
    "            v = f.get_word_vector(w)\n",
    "            vstr = \"\"\n",
    "            for vi in v:\n",
    "                vstr += \" \" + str(vi)\n",
    "            try:\n",
    "                file_out.write(w + vstr+'\\n')\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "convert_bin_to_vec('en_model_fb.bin', 'en')\n",
    "convert_bin_to_vec('ru_model_fb.bin', 'ru')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
