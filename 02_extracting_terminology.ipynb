{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting terminology (key words and phrases)\n",
    "\n",
    "In order to build a glossary, we need to know what words and phrases qualify as terms. For that I will be processing the source (EN) part of the parallel corpus, and later look for matches in the target (RU) part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sveta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sveta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%run utility_file     # handles module imports and loading .csv files\n",
    "from utility_file import Preprocess     # custom class for preprocessing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'pi2.csv'\n",
    "source_lang = 'English'\n",
    "target_lang = 'Russian'\n",
    "\n",
    "source_list, target_list = load_separate_corpora_from_csv(path, source_lang, target_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptions = [\n",
    "    'flowerbed',\n",
    "    'building',\n",
    "    ]\n",
    "\n",
    "clean_en_list = [Preprocess(sentence).preprocess_no_lemmatization(exceptions) \\\n",
    "                 for sentence in source_list if len(sentence) <= 80]\n",
    "# only looking at strings < 80 symbols as longer strings are less likely to contain terms and will clutter the corpus\n",
    "clean_en_str = ' '.join(clean_en_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting unigrams (one-word terms)\n",
    "\n",
    "The first step is simple - I will look at the entire corpus and extract all the verbs and nouns using [spaCy morphology analyzer](https://spacy.io/api/morphology)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating sets of all nouns and verbs using spaCy\n",
    "\n",
    "doc = nlp(clean_en_str)\n",
    "nouns = set()\n",
    "verbs = set()\n",
    "ads = set()\n",
    "for token in doc:\n",
    "    if token.pos_ == \"NOUN\":\n",
    "        nouns.add(token.lemma_)\n",
    "    elif token.pos_ == \"VERB\":\n",
    "        if token.text != \"\\'ve\" and token.text != \"s\" and token.text != \"\\'m\" and token.text != \"\\'re\":\n",
    "            if len(token.text) > 2:\n",
    "                verbs.add(token.lemma_)\n",
    "all_pos = nouns.union(verbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I'm extracting potential one-word terms using the [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) algorithm provided by [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting candidate words using TfidfVectorizer\n",
    "\n",
    "stop_words = \"english\"\n",
    "count = TfidfVectorizer(min_df = 5, stop_words=stop_words).fit(clean_en_list)\n",
    "bag_of_words = count.transform(clean_en_list)\n",
    "sum_words = bag_of_words.sum(axis=0)\n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in count.vocabulary_.items()]\n",
    "words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "tfidf_candidates = []\n",
    "for item in words_freq[:700]:      # only looking at 700 most common words\n",
    "    tfidf_candidates.append(item[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I am only interested in nouns and verbs at this stage, I'm creating the final list of unigram terms by intersecting the set of nouns and terms created previously and the list of tf-idf candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['island', 'level', 'gift', 'open', 'upgrade', 'build', 'collect', 'hotel', 'complete', 'reward', 'set', 'time', 'hold', 'profit', 'building', 'union', 'season', 'play', 'holiday', 'bonus', 'help', 'game', 'buy', 'paradise', 'day', 'chest', 'need', 'event', 'beach', 'love', 'received', 'send', 'receive', 'want', 'christmas', 'use', 'magic', 'treasure', 'easter', 'energy', 'win', 'party', 'trade', 'rhee', 'make', 'mano', 'talk', 'snowball', 'year', 'box', 'water', 'great', 'place', 'sea', 'leprechaun', 'offer', 'flag', 'spring', 'workshop', 'good', 'start', 'santa', 'ice', 'look', 'come', 'store', 'friend', 'ghost', 'speed', 'turtilliada', 'pack', 'return', 'like', 'bank', 'festival', 'search', 'let', 'way', 'know', 'journey', 'earn', 'wondershop', 'park', 'sandy', 'competition', 'house', 'achievement', 'free', 'surprise', 'stage', 'castle', 'power', 'flower', 'battle', 'basket', 'kit', 'sweet', 'try', 'mini', 'fail']\n"
     ]
    }
   ],
   "source": [
    "# Intersecting with the list of nouns and verbs\n",
    "candidates = [x for x in tfidf_candidates if x in all_pos]\n",
    "\n",
    "# taking a look\n",
    "print(candidates[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting n-grams (key phrases)\n",
    "\n",
    "The idea here is the same as with unigrams - first, extracting potential terms with a linguistic tool (spaCy [noun chunker](https://spacy.io/usage/linguistic-features#noun-chunks) in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting n-grams using spacy chunker\n",
    "\n",
    "noun_chunks = set(chunk.text.strip().lower() for chunk in doc.noun_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then extracting n-gram key phrases. In this case, I will be using sklearn's [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) rather than TfidfVectorizer, for this algorithm seems to be providing better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting candidate n-grams with CountVectorizer and intersect them with the noun chunks\n",
    "\n",
    "n_gram_range = (2, 3)\n",
    "count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([clean_en_str])\n",
    "ngrams = count.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously, creating the final list of n-gram terms by intersecting the two lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaah eek ook', 'able fight', 'able stop', 'abracadabra hocus pocus', 'abyss speed construction', 'access wondershop', 'accessories buildings', 'accessory building accessory', 'accessory demolish building', 'accessory warehouse', 'accommodate guests', 'accommodate thinking nature', 'accumulated profit', 'accumulation speed arrival', 'achievement achievem', 'achievement points', 'achievement reward', 'achievements kinds achievements', 'acorn luck', 'acorn rich greenes', 'active event', 'active event demolish', 'active members', 'active members players', 'active players', 'actual locket', 'ad win prize', 'additional bonuses', 'adjacent ones', 'administration building', 'administration building staff', 'administrative building', 'advance buy boxes', 'aeronaut cafe', 'aeronaut cafes', 'afraid hammers', 'afraid heights', 'aiden forrester', 'air castle', 'air essence aaaa', 'air essences', 'alalaz celestial chest', 'alex bootman', 'alien athletes', 'alien chest ufologist', 'alien chests', 'alien flag', 'alien gifts', 'alien guest', 'alien monument gift']\n"
     ]
    }
   ],
   "source": [
    "ngram_candidates = list(filter(lambda candidate: candidate in noun_chunks, ngrams))\n",
    "\n",
    "print(ngram_candidates[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining + Pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3820 is the total number of extracted terms\n"
     ]
    }
   ],
   "source": [
    "keywords = set(candidates + ngram_candidates)\n",
    "print(len(keywords), 'is the total number of extracted terms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickling the keyword set\n",
    "\n",
    "import pickle\n",
    "with open('keywords.pkl', 'wb') as f:\n",
    "       pickle.dump(set(keywords), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
