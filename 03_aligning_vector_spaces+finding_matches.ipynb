{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aligning Vector Spaces\n",
    "\n",
    "In order to match the extracted one-word EN terms with their RU translations, I will be using the vector space alignment method as it is described here:\n",
    "https://github.com/babylonhealth/fastText_multilingual\n",
    "align_your_own.ipynb,\n",
    "fasttext.py\n",
    "\n",
    "The idea is to align the two vector spaces using anchors (a small dictionary of basic EN words and their RU translations) and then look for matches with cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sveta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sveta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "%run utility_file    # handles module imports and loading .csv files\n",
    "from utility_file import Preprocess     # custom class for preprocessing text\n",
    "import fasttext\n",
    "import operator\n",
    "\n",
    "def normalized(a, axis=-1, order=2):\n",
    "    \"\"\"Utility function to normalize the rows of a numpy array.\"\"\"\n",
    "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
    "    l2[l2==0] = 1\n",
    "    return a / np.expand_dims(l2, axis)\n",
    "\n",
    "def make_training_matrices(source_dictionary, target_dictionary, bilingual_dictionary):\n",
    "    \"\"\"\n",
    "    Source and target dictionaries are the FastVector objects of\n",
    "    source/target languages. bilingual_dictionary is a list of \n",
    "    translation pair tuples [(source_word, target_word), ...].\n",
    "    \"\"\"\n",
    "    source_matrix = []\n",
    "    target_matrix = []\n",
    "\n",
    "    for (source, target) in bilingual_dictionary:\n",
    "        if source in source_dictionary and target in target_dictionary:\n",
    "            source_matrix.append(source_dictionary[source])\n",
    "            target_matrix.append(target_dictionary[target])\n",
    "\n",
    "    # return training matrices\n",
    "    return np.array(source_matrix), np.array(target_matrix)\n",
    "\n",
    "def learn_transformation(source_matrix, target_matrix, normalize_vectors=True):\n",
    "    \"\"\"\n",
    "    Source and target matrices are numpy arrays, shape\n",
    "    (dictionary_length, embedding_dimension). These contain paired\n",
    "    word vectors from the bilingual dictionary.\n",
    "    \"\"\"\n",
    "    # optionally normalize the training vectors\n",
    "    if normalize_vectors:\n",
    "        source_matrix = normalized(source_matrix)\n",
    "        target_matrix = normalized(target_matrix)\n",
    "\n",
    "    # perform the SVD\n",
    "    product = np.matmul(source_matrix.transpose(), target_matrix)\n",
    "    U, s, V = np.linalg.svd(product)\n",
    "\n",
    "    # return orthogonal transformation which aligns source language to the target\n",
    "    return np.matmul(U, V)\n",
    "\n",
    "class FastVector:\n",
    "    def __init__(self, vector_file='', transform=None):\n",
    "        self.word2id = {}\n",
    "        self.id2word = []\n",
    "\n",
    "        print('reading word vectors from %s' % vector_file)\n",
    "        with open(vector_file, 'r') as f:\n",
    "            (self.n_words, self.n_dim) = \\\n",
    "                (int(x) for x in f.readline().rstrip('\\n').split(' '))\n",
    "            self.embed = np.zeros((self.n_words, self.n_dim))\n",
    "            for i, line in enumerate(f):\n",
    "                elems = line.rstrip('\\n').split(' ')\n",
    "                self.word2id[elems[0]] = i\n",
    "                self.embed[i] = elems[1:self.n_dim+1]\n",
    "                self.id2word.append(elems[0])\n",
    "\n",
    "        if transform is not None:\n",
    "            print('Applying transformation to embedding')\n",
    "            self.apply_transform(transform)\n",
    "\n",
    "    def apply_transform(self, transform):\n",
    "        transmat = np.loadtxt(transform) if isinstance(transform, str) else transform\n",
    "        self.embed = np.matmul(self.embed, transmat)\n",
    "\n",
    "    def export(self, outpath):\n",
    "\n",
    "        fout = open(outpath, \"w\")\n",
    "\n",
    "        fout.write(str(self.n_words) + \" \" + str(self.n_dim) + \"\\n\")\n",
    "        for token in self.id2word:\n",
    "            vector_components = [\"%.6f\" % number for number in self[token]]\n",
    "            vector_as_string = \" \".join(vector_components)\n",
    "\n",
    "            out_line = token + \" \" + vector_as_string + \"\\n\"\n",
    "            fout.write(out_line)\n",
    "\n",
    "        fout.close()\n",
    "    def translate_nearest_neighbour(self, source_vector):\n",
    "        \"\"\"Obtain translation of source_vector using nearest neighbour retrieval\"\"\"\n",
    "        similarity_vector = np.matmul(FastVector.normalised(self.embed), source_vector)\n",
    "        target_id = np.argmax(similarity_vector)\n",
    "        return self.id2word[target_id]\n",
    "\n",
    "    def translate_inverted_softmax(self, source_vector, source_space, nsamples,\n",
    "                                   beta=10., batch_size=100, recalculate=True):\n",
    "        \"\"\"\n",
    "        Obtain translation of source_vector using sampled inverted softmax retrieval\n",
    "        with inverse temperature beta.\n",
    "        nsamples vectors are drawn from source_space in batches of batch_size\n",
    "        to calculate the inverted softmax denominators.\n",
    "        Denominators from previous call are reused if recalculate=False. This saves\n",
    "        time if multiple words are translated from the same source language.\n",
    "        \"\"\"\n",
    "        embed_normalised = FastVector.normalised(self.embed)\n",
    "        # calculate contributions to softmax denominators in batches\n",
    "        # to save memory\n",
    "        if self.softmax_denominators is None or recalculate is True:\n",
    "            self.softmax_denominators = np.zeros(self.embed.shape[0])\n",
    "            while nsamples > 0:\n",
    "                # get batch of randomly sampled vectors from source space\n",
    "                sample_vectors = source_space.get_samples(min(nsamples, batch_size))\n",
    "                # calculate cosine similarities between sampled vectors and\n",
    "                # all vectors in the target space\n",
    "                sample_similarities = \\\n",
    "                    np.matmul(embed_normalised,\n",
    "                              FastVector.normalised(sample_vectors).transpose())\n",
    "                # accumulate contribution to denominators\n",
    "                self.softmax_denominators \\\n",
    "                    += np.sum(np.exp(beta * sample_similarities), axis=1)\n",
    "                nsamples -= batch_size\n",
    "        # cosine similarities between source_vector and all target vectors\n",
    "        similarity_vector = np.matmul(embed_normalised,\n",
    "                                      source_vector/np.linalg.norm(source_vector))\n",
    "        # exponentiate and normalise with denominators to obtain inverted softmax\n",
    "        softmax_scores = np.exp(beta * similarity_vector) / \\\n",
    "                         self.softmax_denominators\n",
    "        # pick highest score as translation\n",
    "        target_id = np.argmax(softmax_scores)\n",
    "        return self.id2word[target_id]\n",
    "\n",
    "    def get_samples(self, nsamples):\n",
    "        \"\"\"Return a matrix of nsamples randomly sampled vectors from embed\"\"\"\n",
    "        sample_ids = np.random.choice(self.embed.shape[0], nsamples, replace=False)\n",
    "        return self.embed[sample_ids]\n",
    "    \n",
    "    @classmethod\n",
    "    def normalised(cls, mat, axis=-1, order=2):\n",
    "        \"\"\"Utility function to normalise the rows of a numpy array.\"\"\"\n",
    "        norm = np.linalg.norm(\n",
    "            mat, axis=axis, ord=order, keepdims=True)\n",
    "        norm[norm == 0] = 1\n",
    "        return mat / norm\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def cosine_similarity(cls, vec_a, vec_b):\n",
    "        \"\"\"Compute cosine similarity between vec_a and vec_b\"\"\"\n",
    "        return np.dot(vec_a, vec_b) / \\\n",
    "            (np.linalg.norm(vec_a) * np.linalg.norm(vec_b))\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        return key in self.word2id\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.embed[self.word2id[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_anchors(path):\n",
    "    '''\n",
    "    this function loads a .txt file with a bilingual dictionary and turns it into a list of tuples\n",
    "    that will be used as anchors for the transformation\n",
    "    '''\n",
    "\n",
    "    file = open(path, 'r', encoding=\"utf-8\")\n",
    "    text = file.read()\n",
    "    word_list = text.split()\n",
    "    anchors = []\n",
    "    for ind in range(len(word_list)):\n",
    "        if ind % 2 == 0:\n",
    "            pair = (word_list[ind+1], word_list[ind])\n",
    "            anchors.append(pair)\n",
    "    file.close()\n",
    "    \n",
    "    return anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading word vectors from en_model_converted.vec\n",
      "reading word vectors from ru_model_converted.vec\n"
     ]
    }
   ],
   "source": [
    "# Loading the previously saved models and the achor file\n",
    "\n",
    "en_dictionary = FastVector(vector_file='en_model_converted.vec')\n",
    "ru_dictionary = FastVector(vector_file='ru_model_converted.vec')\n",
    "enru_anchors = load_anchors('ru-en.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form the training matrices\n",
    "source_matrix, target_matrix = make_training_matrices(en_dictionary, ru_dictionary, enru_anchors)\n",
    "\n",
    "# learn and apply the transformation\n",
    "transform = learn_transformation(source_matrix, target_matrix)\n",
    "en_dictionary.apply_transform(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I check out how the system works - I enter any random word and see what translation the system finds for it, if finds any at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Look up word or say stop: beautiful\n",
      "настрой\n",
      "Look up word or say stop: building\n",
      "здание\n",
      "Look up word or say stop: day\n",
      "день\n",
      "Look up word or say stop: cat\n",
      "котик\n",
      "Look up word or say stop: sun\n",
      "бесконечный\n",
      "Look up word or say stop: mano\n",
      "мано\n",
      "Look up word or say stop: swim\n",
      "штурвал\n",
      "Look up word or say stop: ship\n",
      "знать\n",
      "Look up word or say stop: island\n",
      "остров\n",
      "Look up word or say stop: chest\n",
      "сундук\n",
      "Look up word or say stop: dorin\n",
      "дорин\n",
      "Look up word or say stop: monkey\n",
      "рия\n",
      "Look up word or say stop: workshop\n",
      "настолько\n",
      "Look up word or say stop: stop\n"
     ]
    }
   ],
   "source": [
    "for x in range(100):\n",
    "    word = input('Look up word or say stop: ')\n",
    "    if word == 'stop':\n",
    "        break\n",
    "    else:\n",
    "        try:\n",
    "            print(ru_dictionary.translate_nearest_neighbour(en_dictionary[word]))\n",
    "        except:\n",
    "            print('No such word')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding matches\n",
    "\n",
    "The system clearly doesn't work well, as the test in the previous cell shows. I tried to improve its performance by making it look for the closest matches only among 3 most common words appearing in the parallel target segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading .csv corpus\n",
    "\n",
    "path = 'pi2.csv'\n",
    "source_lang = 'English'\n",
    "target_lang = 'Russian'\n",
    "\n",
    "source_list, target_list = load_separate_corpora_from_csv(path, source_lang, target_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# loading pickled keywords\n",
    "\n",
    "with open('keywords.pkl', 'rb') as f:\n",
    "    keywords = pickle.load(f)\n",
    "\n",
    "# loading pickled clean corpora\n",
    "\n",
    "with open('clean_en_corpus.pkl', 'rb') as f:\n",
    "       clean_en_corpus = pickle.load(f)\n",
    "with open('clean_ru_corpus.pkl', 'rb') as f:\n",
    "       clean_ru_corpus = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I'm building a dictionary where the keys are terms from the EN corpus and values are top 3 most common words in the parallel segments of the RU corpus. To narrow down the pool further, I will be only looking at words of the same POS (part of speech)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "noun_list = ['NOUN']\n",
    "verb_list = ['VERB', 'INFN']\n",
    "ad_list = ['ADJ', 'ADJF', 'ADJS', 'ADVB', 'ADV', 'ADVB']     # both adverbs and adjectives\n",
    "\n",
    "short_keywords = [keyword for keyword in keywords if len(keyword.split()) == 1]     # only looking at unigrams\n",
    "clean_corpus_as_dict = dict(zip(clean_en_corpus, clean_ru_corpus))\n",
    "keyword_pool_dict = {}\n",
    "for keyword in short_keywords:\n",
    "    source_pos = nlp(keyword)[0].pos_\n",
    "    if source_pos in noun_list:\n",
    "        pos = noun_list\n",
    "    if source_pos in verb_list:\n",
    "        pos = verb_list\n",
    "    if source_pos in ad_list:\n",
    "        pos = ad_list\n",
    "    target_pool_sent = []\n",
    "    for source_sent in clean_corpus_as_dict.keys():\n",
    "        if (keyword in source_sent.split()) and (len(source_sent) < 80):   # only looking at strings < 80 symbols  \n",
    "            target_pool_sent.append(clean_corpus_as_dict[source_sent])         # as longer strings are less likely to contain terms and will clutter the corpus\n",
    "    target_pool_words = ' '.join(target_pool_sent).split()\n",
    "    target_pool_pos = [word for word in target_pool_words if str(morph.parse(word)[0].tag.POS) in pos]\n",
    "    most_common = Counter(target_pool_pos).most_common(3)\n",
    "    keyword_pool_dict[keyword] = [key for key, val in most_common] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sky</th>\n",
       "      <td>небо</td>\n",
       "      <td>кристалл</td>\n",
       "      <td>дракон</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flower</th>\n",
       "      <td>цвет</td>\n",
       "      <td>цветок</td>\n",
       "      <td>корзинка</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>banish</th>\n",
       "      <td>прогнать</td>\n",
       "      <td>прогонять</td>\n",
       "      <td>получить</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bundle</th>\n",
       "      <td>комплект</td>\n",
       "      <td>снег</td>\n",
       "      <td>шиворот</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lantern</th>\n",
       "      <td>отправить</td>\n",
       "      <td>получить</td>\n",
       "      <td>украсить</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>время</td>\n",
       "      <td>награда</td>\n",
       "      <td>прибыль</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>safe</th>\n",
       "      <td>безопасный</td>\n",
       "      <td>точно</td>\n",
       "      <td>любой</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monkey</th>\n",
       "      <td>обезьянка</td>\n",
       "      <td>сезон</td>\n",
       "      <td>игрок</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>golem</th>\n",
       "      <td>голем</td>\n",
       "      <td>голь</td>\n",
       "      <td>сумка</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bring</th>\n",
       "      <td>приносить</td>\n",
       "      <td>привезти</td>\n",
       "      <td>принести</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>501 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0          1         2\n",
       "sky            небо   кристалл    дракон\n",
       "flower         цвет     цветок  корзинка\n",
       "banish     прогнать  прогонять  получить\n",
       "bundle     комплект       снег   шиворот\n",
       "lantern   отправить   получить  украсить\n",
       "...             ...        ...       ...\n",
       "time          время    награда   прибыль\n",
       "safe     безопасный      точно     любой\n",
       "monkey    обезьянка      сезон     игрок\n",
       "golem         голем       голь     сумка\n",
       "bring     приносить   привезти  принести\n",
       "\n",
       "[501 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taking a look at the pool\n",
    "\n",
    "keyword_pool_df = pd.DataFrame.from_dict(keyword_pool_dict, orient='index')\n",
    "keyword_pool_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now from the pool above I'll be picking the word with the highest cosine similarity to the source term\n",
    "\n",
    "termbase = {}\n",
    "for source_term in keyword_pool_dict.keys():\n",
    "    cos_dict = {}\n",
    "    for target_term in keyword_pool_dict[source_term]:\n",
    "        try:\n",
    "            ru_vector = ru_dictionary[target_term]\n",
    "            cos_dict[target_term] = FastVector.cosine_similarity(en_dictionary[source_term], ru_vector)\n",
    "        except:\n",
    "            continue\n",
    "    for item in cos_dict:\n",
    "        try:\n",
    "            translation = max(cos_dict.items(), key=operator.itemgetter(1))[0]\n",
    "            termbase[source_term] = translation\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sky</th>\n",
       "      <td>кристалл</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flower</th>\n",
       "      <td>цветок</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>banish</th>\n",
       "      <td>прогнать</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bundle</th>\n",
       "      <td>комплект</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lantern</th>\n",
       "      <td>украсить</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>награда</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>safe</th>\n",
       "      <td>точно</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monkey</th>\n",
       "      <td>обезьянка</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>golem</th>\n",
       "      <td>голь</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bring</th>\n",
       "      <td>привезти</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>487 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "sky       кристалл\n",
       "flower      цветок\n",
       "banish    прогнать\n",
       "bundle    комплект\n",
       "lantern   украсить\n",
       "...            ...\n",
       "time       награда\n",
       "safe         точно\n",
       "monkey   обезьянка\n",
       "golem         голь\n",
       "bring     привезти\n",
       "\n",
       "[487 rows x 1 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taking a look\n",
    "\n",
    "termbase_df = pd.DataFrame.from_dict(termbase, orient='index')\n",
    "termbase_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks more or less fine.\n",
    "Now pickling the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('termbase_unigrams.pkl', 'wb') as f:\n",
    "       pickle.dump(termbase, f)\n",
    "with open('termbase_top_3.pkl', 'wb') as f:\n",
    "       pickle.dump(keyword_pool_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
